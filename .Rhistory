points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
plot(reg.summary$adjr2, xlab="Number of variables", ylab="RSS", type="l")
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp", type="l")
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS", type="l")
plot(reg.summary$adjr2, xlab="Number of variables", ylab="RSS", type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp", type="l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
plot(reg.summary$bic, xlab="Number of variables", ylab="BIC", type="l")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
par(mfrow c(2,2))
par(mfrow=c(2,2))
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
coef(regfit.full,6)
regfit.fwd <- regsubsets(Salary~., Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~., Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
set.seed(1)
train <- sample(c(T,F),nrow(Hitters),rep=T)
head(train)
test(!train)
train <- sample(c(T,F),nrow(Hitters),rep=T)
test(!train)
test <- (!train)
head(test)
regfit.full <- regsubsets(Salary~., data=Hitters[train,])
test.mat <- model.matrix(Salary~., data=Hitters[test.mat])
test.mat=model.matrix(Salary~., data=Hitters[test.mat])
test.mat=model.matrix(Salary~.,data=Hitters[test,])
test.mat <- model.matrix(Salary~., data=Hitters[test,])
val.errors <- rep(NA,19)
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.full, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
regfit.best <- regsubsets(Salary~., data=Hitters[train,])
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
set.seed(1)
train <- sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
head(train)
test <- (!train)
head(test)
regfit.best <- regsubsets(Salary~., data=Hitters[train,])
test.mat <- model.matrix(Salary~., data=Hitters[test,])
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
regfit.best <- regsubsets(Salary~., data=Hitters[train,],nvmax=19)
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best,7)
install.packages("glmnet")
library(glmnet)
x <- model.matrix(Salary~.,Hitters[,-1])
y <- Hitters$Salary
grid <- 10^seq(10,-2,length=100)
ridge.mod <- glmnet(x,y,lambda = grid, alpha = 0)
dim(ridge.mod)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
dim(train)
head(train)
length(train)
test <- (-train)
y.test <- y[test]
ridge.mod <- glmnet(x[train,],y[train],alpha = 0, lambda=grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod,s=4,newx = x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred <- predict(ridge.mod,s=1e10,newx = x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred <- predict(ridge.mod,s=0,newx = x[test,], exact=T)
lm(y~x, subset=train)
cv.out <- cv.glmnet(x[train,],y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2)
out <- glmnet(x,y,alpha = 0)
predict(out, type='coefficients', s=bestlam,)[1:20]
predict(out, type='coefficients', s=bestlam,)[1:20,]
lasso.mod <- glmnet(x[train,],y[train],alpha = 1, lambda=grid)
plot(lasso.mod)
plot(lasso.mod)
lasso.mod <- glmnet(x[train,],y[train],alpha = 1, lamdda=grid)
plot(lasso.mod)
lasso.mod <- glmnet(x[train,],y[train],alpha = 1, lamdda=grid)
plot(lasso.mod)
library(glmnet)
x <- model.matrix(Salary~.,Hitters[,-1])
y <- Hitters$Salary
#alpha = 0, ridge
#alpha = 1, lasso
grid <- 10^seq(10,-2,length=100)
ridge.mod <- glmnet(x,y,lamdda = grid, alpha = 0)
dim(coef(ridge.mod))
ridge.mod$lamdda[50]
coef(ridge.mod)[,50]
ridge.mod$lamdda[60]
coef(ridge.mod)[,60]
set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
head(train)
length(train)
test <- (-train)
y.test <- y[test]
ridge.mod <- glmnet(x[train,],y[train],alpha = 0, lamdda=grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod,s=4,newx = x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred <- predict(ridge.mod,s=1e10,newx = x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
#use cross validation to find tuning parameter with cv.glmnet
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lamdda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2)
out <- glmnet(x,y,alpha = 0)
predict(out, type='coefficients', s=bestlam,)[1:20,]
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
attach(Hitters)
library(leaps)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
regfit.full <- regsubsets(Salary~., Hitters)
summary(regfit.full)
regfit.full <- regsubsets(Salary~., Hitters, nvmax=19)
reg.summary <- summary(regfit.full)
summary(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS", type="l")
plot(reg.summary$adjr2, xlab="Number of variables", ylab="RSS", type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp", type="l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
plot(reg.summary$bic, xlab="Number of variables", ylab="BIC", type="l")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
par(mfrow=c(2,2))
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
coef(regfit.full,6)
regfit.fwd <- regsubsets(Salary~., Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~., Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
set.seed(1)
train <- sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
head(train)
test <- (!train)
head(test)
regfit.best <- regsubsets(Salary~., data=Hitters[train,],nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[test,])
val.errors <- rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best, id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best,7)
library(glmnet)
x <- model.matrix(Salary~.,Hitters[,-1])
y <- Hitters$Salary
#alpha = 0, ridge
#alpha = 1, lasso
grid <- 10^seq(10,-2,length=100)
ridge.mod <- glmnet(x,y,lamdda = grid, alpha = 0)
dim(coef(ridge.mod))
ridge.mod$lamdda[50]
coef(ridge.mod)[,50]
ridge.mod$lamdda[60]
coef(ridge.mod)[,60]
set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
head(train)
length(train)
test <- (-train)
y.test <- y[test]
ridge.mod <- glmnet(x[train,],y[train],alpha = 0, lamdda=grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod,s=4,newx = x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred <- predict(ridge.mod,s=1e10,newx = x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
#use cross validation to find tuning parameter with cv.glmnet
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lamdda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2)
out <- glmnet(x,y,alpha = 0)
predict(out, type='coefficients', s=bestlam,)[1:20,]
lasso.mod <- glmnet(x[train,],y[train],alpha = 1, lamdda=grid)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(x[train],y[train],alpha=1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=1)
best.lam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=best.lam,newx = x[test,])
mean((lasso.pred-y.test)^2)
out <- glmnet(x,y,alpha=1 lamda=grid)
out <- glmnet(x,y,alpha=1, lamda=grid)
lasso.coef <- predict(out, type='coefficient',s=bestlam)[1:20,]
lasso.coef
install.packages("pls")
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, scale=T, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
set.seed(1)
pcr.fit <- pcr(Salary~.,Hitters, subset=train, scale=T, validation="CV")
pcr.fit <- pcr(Salary~.,data=Hitters[train], scale=T, validation="CV")
pcr.fit <- pcr(Salary~.,data=Hitters[train,], scale=T, validation="CV")
validationplot(pcr.fit, val.type = "MESP")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred <- predict(pc.fit, Hitters[test,] ncomp=5)
pcr.pred <- predict(pc.fit, Hitters[test,], ncomp=5)
pcr.pred <- predict(pcr.fit, Hitters[test,], ncomp=5)
mean((pcr.pred-Hitters$Salary[test])^2)
pcr.pred <- predict(pcr.fit, Hitters[test,], ncomp=7)
mean((pcr.pred-Hitters$Salary[test])^2)
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters[train,], scale=T, validation = "CV")
summary(pls.fit)
pls.pred <- predict(pls.fit, Hitters[test,] ncomp=2)
pls.pred <- predict(pls.fit, Hitters[test,], ncomp=2)
mean((pls.pred-Hitters$Salary[test])^2)
pls.fit <- plsr(Salar~.,data = HItters, scale = T, ncomp=2)
pls.fit <- plsr(Salar~.,data = Hitters, scale = T, ncomp=2)
pls.fit <- plsr(Salary~.,data = Hitters, scale = T, ncomp=2)
summary(pls.fit)
?leaps
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(pls)
library(glmnet)
library(leaps)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(pls)
library(glmnet)
library(leaps)
attach(Boston)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(MASS)
library(pls)
library(glmnet)
library(leaps)
attach(Boston)
attach(College)
set.seed(1)
train <- sample(c(T,F),nrow(College),rep=T)
test <- !train
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(MASS)
library(pls)
library(glmnet)
library(leaps)
attach(Boston)
attach(College)
set.seed(1)
train <- sample(c(T,F),nrow(College),rep=T)
test <- !train
lm.fit <- lm(Apps~., data=train)
lm.fit <- lm(Apps~., data=College$train)
lm.fit <- lm(Apps~., data=train)
lm.fit <- lm(Apps~., subset=train)
lm.fit <- lm(Apps~., data=College)
summary(lm.fit)
lm.pred <- predict(lm.fit, newdata=test)
lm.pred <- predict(lm.fit, test)
lm.pred <- predict(lm.fit, as.data.frame()test)
lm.pred <- predict(lm.fit, as.data.frame(test))
lm.err <- mean((test$Apps-lm.pred)^2)
lm.err <- mean((Apps-lm.pred)^2)
lm.err
lm.fit <- lm(Apps~., data=College)
summary(lm.fit)
lm.pred <- predict(lm.fit, as.data.frame(test))
lm.err <- mean((Apps-lm.pred)^2)
lm.err
xtrain <- model.matrix(Apps~., data=train[,-1])
train <- sample(1:nrow(x),nrow(x)/2)
xtrain <- model.matrix(Apps~., data=train[,-1])
ytrain <- train$apps
ytrain <- train$Apps
ytrain <- Apps
xtest <- model.matrix(Apps~., data=test[,-1])
ytest <- Apps
ridge.fit <- cv.glmnet(xtrain,ytrain,alpha=0)
xtrain <- model.matrix(Apps~., data=train[,])
xtrain <- model.matrix(Apps~., data=train)
xtrain <- model.matrix(Apps~., train)
xtrain <- model.matrix(Apps~., data=train)
train <- sample(c(T,F),nrow(College),rep=T)
test <- !train
xtrain <- model.matrix(Apps~., data=train)
xtrain <- model.matrix(Apps~., data=train[,])
dim(train)
train <- sample(c(T,F),nrow(Apps),rep=T)
train <- sample(c(TRUE,FALSE),nrow(Apps),rep=TRUE)
train <- sample(c(TRUE,FALSE),nrow(College$Apps),rep=TRUE)
set.seed(1)
trainindex <- sample(nrow(College, 0.75*nrow(College)))
set.seed(1)
trainindex <- sample(nrow(College), 0.75*nrow(College))
head(trainindex)
train <- College[trainindex,]
test <- College[-trainingdex,]
set.seed(1)
trainindex <- sample(nrow(College), 0.75*nrow(College))
head(trainindex)
train <- College[trainindex,]
test <- College[-trainindex,]
dim(College)
dim(train)
dim(test)
lm.fit <- lm(Apps~., data=College)
summary(lm.fit)
lm.pred <- predict(lm.fit, newdata=test)
lm.err <- mean((Apps-lm.pred)^2)
lm.err
lm.err
lm.fit <- lm(Apps~., data=train)
summary(lm.fit)
lm.pred <- predict(lm.fit, newdata=test)
lm.err <- mean((Apps-lm.pred)^2)
lm.err
lm.err <- mean((test$Apps-lm.pred)^2)
lm.err
xtrain <- model.matrix(Apps~., data=train[,-1])
ytrain <- Apps
xtest <- model.matrix(Apps~., data=test[,-1])
ytest <- Apps
ridge.fit <- cv.glmnet(xtrain,ytrain,alpha=0)
xtrain <- model.matrix(Apps~., data=train[,-1])
ytrain <- test$Apps
xtest <- model.matrix(Apps~., data=test[,-1])
ytest <- test$Apps
ridge.fit <- cv.glmnet(xtrain,ytrain,alpha=0)
xtrain <- model.matrix(Apps~., data=train[,-1])
ytrain <- train$Apps
xtest <- model.matrix(Apps~., data=test[,-1])
ytest <- test$Apps
ridge.fit <- cv.glmnet(xtrain,ytrain,alpha=0)
plot(ridge.fit)
ridge.lambda <- ridge.fit$lambda.min
ridge.lambda
ridge.pred <- predict(ridge.fit, s=ridge.lambda, newx = xtest)
ridge.err <- mean((ridge.pred-ytest)^2)
ridge.err
lasso.fit <- cv.glmnet(xtrain, ytrain, alpha=1)
plot(lasso.fit)
lasso.lambda <- lasso.fit&lambda.min
lasso.lambda <- lasso.fit$lambda.min
lasso.lambda
lasso.pred <- predict(lasso.fit, s=lasso.lambda, newx= xtest)
lasso.err <- mean((lasso.pred-ytest)^2)
lasso.err
lasso.lambda <- lasso.fit$lambda.min
lasso.lambda
lasso.pred <- predict(lasso.fit, s=lasso.lambda, newx= xtest)
lasso.err <- mean((lasso.pred-ytest)^2)
lasso.err
lasso.coef <- predict(lasso.fit, type="coeffefficients", s=lasso.lambda[1:18,])
lasso.coef <- predict(lasso.fit, type="coeffefficients", s=lasso.lambda)[1:18,]
lasso.coef <- predict(lasso.fit, type="coefficients", s=lasso.lambda)[1:18,]
lasso.coef
pcr.fit <- pcr(Apps~., data=train, scale=T, validation="CV")
validationplot(pcr.fit, val.typ="MSEP")
summary(pcr.fit)
pcr.pred <- predict(pcr.fit, test, ncom=17)
pcr.err <- mean((pcr.pred-test$Apps)^2)
pcr.err
pls.fit <- plsr(Apps~., data=train, scale=T, validation="CV")
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)
pls.pred <- predict(pls.fit, test, ncomp=9)
pls.err <- mean((pls.pred-test$Apps)^2)
pls.err
predict.regsubsets <- function(object, newdata,id,...) {
form=as.formula(object$call[2])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
mat[, names(coefi)] %*% coefi
}
k <- 10
p <- ncol(Boston)-1
folds <- sample(rep(1:k, length = nrow(Boston)))
cv.errors <- matrix(NA,k,p)
for (i in 1:k) {
best.fit <- regsubsets(crim~., data=Boston[folds ~=i,], nvmax=p)
predict.regsubsets <- function(object, newdata,id,...) {
form=as.formula(object$call[2])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
mat[, names(coefi)] %*% coefi
}
k <- 10
p <- ncol(Boston)-1
folds <- sample(rep(1:k, length = nrow(Boston)))
cv.errors <- matrix(NA,k,p)
for (i in 1:k) {
best.fit <- regsubsets(crim~., data=Boston[folds !=i,], nvmax=p)
for (j in 1:p) {
pred <- predict(best.fit, Boston[folds ==i,], id=j)
cv.errors[i,j] <- mean((Boston$crim[folds == i]-pred)^2)
}
}
predict.regsubsets <- function(object, newdata,id,...) {
form=as.formula(object$call[2])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
mat[, names(coefi)] %*% coefi
}
k <- 10
p <- ncol(Boston)-1
folds <- sample(rep(1:k, length = nrow(Boston)))
cv.errors <- matrix(NA,k,p)
for (i in 1:k) {
best.fit <- regsubsets(crim~., data=Boston[folds !=i,], nvmax=p)
for (j in 1:p) {
pred <- predict(best.fit, Boston[folds ==i,], id=j)
cv.errors[i,j] <- mean((Boston$crim[folds == i]-pred)^2)
}
}
predict.regsubsets <- function(object, newdata,id,...) {
form=as.formula(object$call[2])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
mat[, names(coefi)] %*% coefi
}
k <- 10
p <- ncol(Boston)-1
folds <- sample(rep(1:k, length = nrow(Boston)))
cv.errors <- matrix(NA,k,p)
for (i in 1:k) {
best.fit = regsubsets(crim~., data=Boston[folds !=i,], nvmax=p)
for (j in 1:p) {
pred <- predict(best.fit, Boston[folds ==i,], id=j)
cv.errors[i,j] <- mean((Boston$crim[folds == i]-pred)^2)
}
}
