fwd <- regsubsets(Outstate ~ ., data=train, nvmax=17, method='forward')
fwd_sum <- summary(fwd)
par(mfrow=c(2,2))
plot(fwd_sum$cp ,xlab="Number of Variables ", ylab="Cp",
type="b")
points(which.min(fwd_sum$cp), fwd_sum$cp[which.min(fwd_sum$cp)], col="red", cex=2, pch=20)
plot(fwd_sum$bic ,xlab="Number of Variables ",
ylab="BIC",type="b")
points(which.min(fwd_sum$bic), fwd_sum$bic[which.min(fwd_sum$bic)], col="red", cex=2, pch=20)
plot(fwd_sum$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted R^2^",type="b")
points(which.max(fwd_sum$adjr2), fwd_sum$adjr2[which.max(fwd_sum$adjr2)], col="red", cex=2, pch=20)
which.min(fwd_sum$cp)
which.min(fwd_sum$bic)
which.max(fwd_sum$adjr2)
test_matrix <- model.matrix(Outstate~., data=test)
val.errors <- rep(NA,17)
for(i in 1:17){
coefi <- coef(fwd,id=i)
pred <- test_matrix[,names(coefi)]%*%coefi
val.errors[i] <- mean((test$Outstate-pred)^2)
}
which.min(val.errors)
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[12], col='red', pch=20, cex=2)
fwd_full <- regsubsets(Outstate ~ ., data=College, nvmax=17, method='forward')
coef(fwd_full, 6)
gam_fit <- gam(Outstate ~ Private + s(Room.Board, 3) + s(Terminal, 3) + s(perc.alumni, 3) + s(Expend, 3) + s(Grad.Rate, 3), data=train)
par(mfrow=c(2,3))
plot(gam_fit, se=TRUE, col="blue")
preds <- predict(gam_fit, newdata = test)
error <- mean((test$Outstate-preds)^2)
val.errors[6]-error
summary(gam_fit)
attach(Wage)
set.seed(1)
cv.error <- rep(0,5)
for (i in 1:5){
glm.fit <- glm(wage ~ poly(age,i),data=Wage)
cv.error[i]<- cv.glm(Wage,glm.fit,K=10)$delta[1]
}
cv.error
plot(cv.error, type="b", xlab="Degree", ylab="Test MSE")
points(which.min(cv.error), cv.error[4], col="red", pch=20, cex=2)
fit_1 <- lm(wage ~ age, data=Wage)
fit_2 <- lm(wage ~ poly(age, 2), data=Wage)
fit_3 <- lm(wage ~ poly(age, 3), data=Wage)
fit_4 <- lm(wage ~ poly(age, 4), data=Wage)
fit_5 <- lm(wage ~ poly(age, 5), data=Wage)
anova(fit_1, fit_2, fit_3, fit_4, fit_5)
age_lim <- range(age)
age_grid <- seq(from=age_lim[1], to=age_lim[2])
preds <- predict(fit_4, newdata=list(age=age_grid),se=TRUE)
se_bands <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
plot(age, wage, xlim=age_lim, cex=.5, col="darkgrey")
lines(age_grid, preds$fit, lwd=2, col="blue")
matlines(age_grid, se_bands, lwd=1, col="blue", lty=3)
set.seed(2)
cv.errors <- rep(NA, 10)
for(i in 2:10){
Wage$age.cut <- cut(Wage$age,i)
glm.fit <- glm(wage ~ age.cut, data=Wage)
cv.errors[i] <- cv.glm(Wage, glm.fit, K=10)$delta[1]
}
cv.errors
plot(2:10, cv.errors[-1], type="b", xlab="Number of cuts", ylab="Test MSE")
points(which.min(cv.errors), cv.errors[which.min(cv.errors)], col="red", pch=20, cex=2)
fit_step <- glm(wage ~ cut(age, 8), data=Wage)
preds <- predict(fit_step, data.frame(age = age_grid))
plot(age, wage, col="darkgray")
lines(age_grid, preds, col="darkgreen", lwd=2)
set.seed(10)
test_sample <- sample(1:nrow(College), nrow(College)/4)
train <- College[-test_sample, ]
test <- College[test_sample, ]
fwd <- regsubsets(Outstate ~ ., data=train, nvmax=17, method='forward')
fwd_sum <- summary(fwd)
par(mfrow=c(2,2))
plot(fwd_sum$cp ,xlab="Number of Variables ", ylab="Cp",
type="b")
points(which.min(fwd_sum$cp), fwd_sum$cp[which.min(fwd_sum$cp)], col="red", cex=2, pch=20)
plot(fwd_sum$bic ,xlab="Number of Variables ",
ylab="BIC",type="b")
points(which.min(fwd_sum$bic), fwd_sum$bic[which.min(fwd_sum$bic)], col="red", cex=2, pch=20)
plot(fwd_sum$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted R^2^",type="b")
points(which.max(fwd_sum$adjr2), fwd_sum$adjr2[which.max(fwd_sum$adjr2)], col="red", cex=2, pch=20)
which.min(fwd_sum$cp)
which.min(fwd_sum$bic)
which.max(fwd_sum$adjr2)
test_matrix <- model.matrix(Outstate~., data=test)
val.errors <- rep(NA,17)
for(i in 1:17){
coefi <- coef(fwd,id=i)
pred <- test_matrix[,names(coefi)]%*%coefi
val.errors[i] <- mean((test$Outstate-pred)^2)
}
which.min(val.errors)
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[12], col='red', pch=20, cex=2)
set.seed(10)
test_sample <- sample(1:nrow(College), nrow(College)/4)
train <- College[-test_sample, ]
test <- College[test_sample, ]
fwd <- regsubsets(Outstate ~ ., data=train, nvmax=17, method='forward')
fwd_sum <- summary(fwd)
par(mfrow=c(2,2))
plot(fwd_sum$cp ,xlab="Number of Variables ", ylab="Cp",
type="b")
points(which.min(fwd_sum$cp), fwd_sum$cp[which.min(fwd_sum$cp)], col="red", cex=2, pch=20)
plot(fwd_sum$bic ,xlab="Number of Variables ",
ylab="BIC",type="b")
points(which.min(fwd_sum$bic), fwd_sum$bic[which.min(fwd_sum$bic)], col="red", cex=2, pch=20)
plot(fwd_sum$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted R^2^",type="b")
points(which.max(fwd_sum$adjr2), fwd_sum$adjr2[which.max(fwd_sum$adjr2)], col="red", cex=2, pch=20)
which.min(fwd_sum$cp)
which.min(fwd_sum$bic)
which.max(fwd_sum$adjr2)
test_matrix <- model.matrix(Outstate~., data=test)
val.errors <- rep(NA,17)
for(i in 1:17){
coefi <- coef(fwd,id=i)
pred <- test_matrix[,names(coefi)]%*%coefi
val.errors[i] <- mean((test$Outstate-pred)^2)
}
which.min(val.errors)
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[13], col='red', pch=20, cex=2)
fwd_full <- regsubsets(Outstate ~ ., data=College, nvmax=17, method='forward')
coef(fwd_full, 5)
gam_fit <- gam(Outstate ~ Private + s(Room.Board, 3) + s(Terminal, 3) +
s(perc.alumni, 3) + s(Expend, 3) + s(Grad.Rate, 3), data=train)
par(mfrow=c(2,3))
plot(gam_fit, se=TRUE, col="blue")
preds <- predict(gam_fit, newdata = test)
error <- mean((test$Outstate-preds)^2)
val.errors[5]-error
summary(gam_fit)
install.packages("rpart")
library(ISLR)
library(caret)
library(caret)
library(rpart)
attach(Sales)
attach(sales)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
High <- as.factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
table(High)
Carseats <- data.frame(Carseats[,-1], High)
str(Carseats)
tree.carseats <- rpart(High~., data=Carseats, method="class", control=rpart.control(minsplit = 15, cp=0.1))
summary(tree.carseats
summary(tree.carseats)
summary(tree.carseats)
install.packages("rattle")
library(rattle)
fancyRpartPlot(tree.carseats)
tree.carseats
printcp(tree.carseats)
plotcp(tree.carseats)
tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"],"CP")]
tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"]
carseats.prune <- prune(tree.carseats, cp=tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"])
fancyRpartPlot(carseats.prune)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(MASS)
str(Boston)
train_control <- trainControl(method="repeatedcv", number=10, repeats = 3)
tree.boston <- train(medv~., data=Boston, trControl=train_control, method = "rpart")
tree.boston
tree.boston$finalModel
library(rpart.plot)
rpart.plot(tree.boston$finalModel)
inTrain <- createDataPartition(Boston$medv~., p=0.5, list=F)
inTrain <- createDataPartition(Boston$medv, p=0.5, list=F)
train <- Boston[inTrain,]
boston.rf <- train(medv~., data=train, method="rf", trControl=trainControl("cv", number=10), importance=T)
boston.rf <- train(medv~., data=train, method="rf", trControl=trainControl("cv", number=10), importance=T)
boston.rf$bestTune
boston.rf$finalModel
varImp(boston.rf)
plot(varImp(boston.rf))
gbm.boston <- train(medv~., data=train, distribution = "gaussian", method = "gbm",
trControl = train_control, verbose=F, metric = "RMSE", bag.fraction = 0.75)
train_control <- trainControl(method="cv", number = 10)
gbm.boston <- train(medv~., data=train, distribution = "gaussian", method = "gbm",
trControl = train_control, verbose=F, metric = "RMSE", bag.fraction = 0.75)
print(gbm.boston)
summary(gbm.boston)
par(mfrow=c(1,2))
plot(gbm.boston$finalModel, i="rm")
plot(gbm.boston$finalModel, i="lstat")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(tidyverse)
library(rpart.plot)
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.1)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.01)
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.001)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.01)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- tree(Sales ~ ., data = car_train)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
car_tree <- rpart(carseats$Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
car_tree <- rpart(Carseats$Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
attach(Carseats)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(tidyverse)
library(rpart.plot)
p <- seq(0, 1, 0.1)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
set.seed(1)
attach(Carseats)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
summary(car_tree)
High <- as.factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
table(High)
Carseats <- data.frame(Carseats[,-1], High)
str(Carseats)
##fit the tree
#cp = complexity parameter
tree.carseats <- rpart(High~., data=Carseats, method="class", control=rpart.control(minsplit = 15, cp=0.1))
summary(tree.carseats)
fancyRpartPlot(tree.carseats)
tree.carseats
printcp(tree.carseats)
plotcp(tree.carseats)
tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"]
carseats.prune <- prune(tree.carseats, cp=tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"])
fancyRpartPlot(carseats.prune)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.01))
summary(car_tree)
car_tree <- rpart(Sales ~ ., data = Carseats, method = "class", control = rpart.control(minsplit = 15, cp = 0.01))
summary(car_tree)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.01))
summary(car_tree)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 10, cp = 0.01))
summary(car_tree)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(tidyverse)
library(rpart.plot)
p <- seq(0, 1, 0.1)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
set.seed(1)
attach(Carseats)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- tree(Sales~., data=car_train)
library(tree)
install.packages("tree")
library(tree)
car_tree <- tree(Sales~., data=car_train)
attach(Carseats)
car_tree <- tree(Sales~., data=car_train)
set.seed(1)
attach(Carseats)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- tree(Sales~., data=car_train)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(tidyverse)
library(rpart.plot)
library(tree)
p <- seq(0, 1, 0.1)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
set.seed(1)
attach(Carseats)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = 0)
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = 0)
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = 1)
?text
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = NULL)
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = 0)
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = 10)
car_tree <- tree(Sales~., data=car_train)
summary(car_tree)
plot(car_tree)
text(car_tree, pretty = 0)
plot(car_tree)
text(car_tree, pretty = 0)
car_pred <- predict(car_tree, car_test)
mean((car_test$Sales - car_pred)^2)
car_cv <- cv.tree(car_tree, FUN = prune.tree)
par(mfrow = c(1,2))
plot(car_cv$size, car_cv$dev, type = "b")
plot(car_cv$k, car_cv$dev, type = "b")
car_cv <- cv.tree(car_tree, FUN = prune.tree)
par(mfrow = c(1,2))
plot(car_cv$size, car_cv$dev, type = "b")
plot(car_cv$k, car_cv$dev, type = "b")
car_pruned <- prune.tree(car_tree, best = 14)
par(mfrow = c(1, 1))
plot(car_pruned)
text(car_pruned, pretty = 0)
car_cv <- cv.tree(car_tree, FUN = prune.tree)
par(mfrow = c(1,2))
plot(car_cv$size, car_cv$dev, type = "b")
plot(car_cv$k, car_cv$dev, type = "b")
car_pruned <- prune.tree(car_tree, best = 14)
par(mfrow = c(1, 1))
plot(car_pruned)
text(car_pruned, pretty = 0)
pred_pruned <- predict(car_pruned, car_test)
mean((car_test$Sales - pred_pruned)^2)
library(randomForest)
car_bag <- randomForest(Sales ~ ., data=car_train, mtry = 10, ntree = 500, importance = T)
pred_bag <- predict(car_bag, car_test)
mean((car_test$Sales - pred_bag)^2)
importance(car_bag)
car_rf <- randomForest(Sales ~ ., data = car_train, mtry = 5, ntree = 500, importance = T)
pred_rf <- predict(car_rf, car_test)
mean((car_test$Sales - pref_rf)^2)
car_rf <- randomForest(Sales ~ ., data = car_train, mtry = 5, ntree = 500, importance = T)
pred_rf <- predict(car_rf, car_test)
mean((car_test$Sales - pred_rf)^2)
importance((car_rf))
attach(OJ)
set.seed(40)
train <- sample(dim(OJ)[1], 800)
oj_train <- OJ[train,]
oj_test <- OJ[-train,]
oj_tree <- tree(Purchase ~ ., data = oj_train)
summary(oj_tree)
oj_tree
plot(oj_tree)
text(oj_tree, pretty = 0)
oj_pred <- predict(oj_tree, oj_test, type = "class")
table(oj_test$Purchase, oj_pred)
(148+68)/(148+20+34+68)
1-((148+68)/(148+20+34+68))
cv_oj <- cv.tree(oj_tree, FUN = prune.tree)
cv_oj
plot(cv_oj$size, cv_oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
oj_prune = prune.tree(oj_tree, best = 5)
summary(oj_prune)
1-((152+67)/152+19+32+67)
1-((152+67)/(152+19+32+67))
summary(oj_tree)
summary(oj_prune)
tree_pred <- predict(oj_prune, newdata = oj_test, type = "class")
table(tree_pred, oj_test$Purchase)
1-((144+70)/(144+32+24+70))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
data("PimaIndiansDiabetes2", package="mlbench")
install.packages("mlbench")
data("PimaIndiansDiabetes2", package="mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
data("PimaIndiansDiabetes2", package="mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
sample_n(pima.data, 3)
train_control <- trainControl(method="repeatedcv", number =3, repeats = 3)
svm1 <- train(diabetes~., data=pima.data, method = "svmLinear", trControl = train_control, preProcess = c("center", "scale"))
svm1
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(pima.data, 3)
svm1 <- train(diabetes~., data=pima.data, method = "svmLinear", trControl = train_control, preProcess = c("center", "scale"))
svm1
library(tidyverse)
library(caret)
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(pima.data, 3)
# Set up Repeated k-fold Cross Validation
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# Fit the model
svm1 <- train(diabetes ~., data = pima.data, method = "svmLinear", trControl = train_control,  preProcess = c("center","scale"))
#View the model
svm1
# Fit the model
svm2 <- train(diabetes ~., data = pima.data, method = "svmLinear", trControl = train_control,  preProcess = c("center","scale"), tuneGrid = expand.grid(C = seq(0, 2, length = 20)))
#View the model
svm2
# Plot model accuracy vs different values of Cost
plot(svm2)
# Print the best tuning parameter C that
# maximizes model accuracy
svm2$bestTune
res2<-as_tibble(svm2$results[which.min(svm2$results[,2]),])
res2
# Fit the model
svm3 <- train(diabetes ~., data = pima.data, method = "svmRadial", trControl = train_control, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
svm3$bestTune
#View the model
svm3
#save the results for later
res3<-as_tibble(svm3$results[which.min(svm3$results[,2]),])
res3
# Fit the model
svm4 <- train(diabetes ~., data = pima.data, method = "svmPoly", trControl = train_control, preProcess = c("center","scale"), tuneLength = 4)
# Print the best tuning parameter sigma and C that maximizes model accuracy
svm4$bestTune
#View the model
svm4
#save the results for later
res4<-as_tibble(svm4$results[which.min(svm4$results[,2]),])
res4
df<-tibble(Model=c('SVM Linear','SVM Linear w/ choice of cost','SVM Radial','SVM Poly'),Accuracy=c(svm1$results[2][[1]],res2$Accuracy,res3$Accuracy,res4$Accuracy))
df %>% arrange(Accuracy)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
plot(x1, x2, xlab = "X1", ylab = "X2", col = (4 - y), pch = (3 - y))
set.seed(1)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <-  1 * (x1^2 - x2^2 > 0)
plot(x1, x2, xlab = "X1", ylab = "X2", col = (4 - y), pch = (3 - y))
log_fit <- glm(y ~ x1 + x2, family = "binomial")
summary(log_fit)
data <- data.fram(x1 = c1, x2 = x2, y = y)
data <- data.frame(x1 = c1, x2 = x2, y = y)
data <- data.frame(x1 = x1, x2 = x2, y = y)
lm.prob <- predict(lm.fit, data, type = "response")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(tidyverse)
data <- data.frame(x1 = x1, x2 = x2, y = y)
lm.prob <- predict(lm.fit, data, type = "response")
data <- data.frame(x1 = x1, x2 = x2, y = y)
lm.prob <- predict(lm.fit, data, type = "response")
data <- data.frame(x1 = x1, x2 = x2, y = y)
lm.prob <- predict(lm.fit, data, type = "response")
lm.prob = predict(lm.fit, data, type = "response")
data <- data.frame(x1 = x1, x2 = x2, y = y)
lm.prob <- predict(lm.fit(), data, type = "response")
data <- data.frame(x1 = x1, x2 = x2, y = y)
lm.prob <- predict(lm.fit, data, type = "response")
