se_bands <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
plot(age, wage, xlim=age_lim, cex=.5, col="darkgrey")
lines(age_grid, preds$fit, lwd=2, col="blue")
matlines(age_grid, se_bands, lwd=1, col="blue", lty=3)
set.seed(2)
cv.errors <- rep(NA, 10)
for(i in 2:10){
Wage$age.cut <- cut(Wage$age,i)
glm.fit <- glm(wage ~ age.cut, data=Wage)
cv.errors[i] <- cv.glm(Wage, glm.fit, K=10)$delta[1]
}
cv.errors
plot(2:10, cv.errors[-1], type="b", xlab="Number of cuts", ylab="Test MSE")
points(which.min(cv.errors), cv.errors[which.min(cv.errors)], col="red", pch=20, cex=2)
fit_step <- glm(wage ~ cut(age, 8), data=Wage)
preds <- predict(fit_step, data.frame(age = age_grid))
plot(age, wage, col="darkgray")
lines(age_grid, preds, col="darkgreen", lwd=2)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
attach(Wage)
fitr <- lm(wage~poly(age,4, raw = T), data=Wage)
coef(summary(fitr))
fit <- lm(wage~poly(age,4), data=Wage)
coef(summary(fit))
fitr <- lm(wage~poly(age,4, raw = T), data=Wage)
coef(summary(fitr))
fit2a <- lm(wage~age+I(age^2) + I(age^3) + I(age^4), data = Wage)
coef(fit2a)
fit2b <- lm(wage~cbind(age, age^2, age^3, age^4), data = Wage)
coef(fit2b)
fit <- lm(wage~poly(age,4), data=Wage)
coef(summary(fit))
fitr <- lm(wage~poly(age,4, raw = T), data=Wage)
coef(summary(fitr))
fit2a <- lm(wage~age+I(age^2) + I(age^3) + I(age^4), data = Wage)
coef(fit2a)
fit2b <- lm(wage~cbind(age, age^2, age^3, age^4), data = Wage)
coef(fit2b)
agelims=range(age)
ragnge(age)
range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
pred <- predict(fit, newdata=list(age.grid), se=T)
agelims <- range(age)
range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
pred <- predict(fit, newdata=list(age.grid), se=T)
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
preds <- predict(fit, newdata=list(age.grid), se=T)
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age, grid, preds$fit, lwd=1, col="lightblue", lty=3)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age, grid, preds$fit, lwd=1, col="lightblue", lty=3)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age, grid, preds$fit, lwd=2, col="lightblue"
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age, grid, preds$fit, lwd=4, col="lightblue")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age, grid, preds$fit, lwd=2, col="lightblue")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age.grid, preds$fit, lwd=2, col="lightblue")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age.grid, preds$fit, lwd=2, col="blue")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
attach(Wage)
fit <- lm(wage~poly(age,4), data=Wage)
coef(summary(fit))
fitr <- lm(wage~poly(age,4, raw = T), data=Wage)
coef(summary(fitr))
fit2a <- lm(wage~age+I(age^2) + I(age^3) + I(age^4), data = Wage)
coef(fit2a)
fit2b <- lm(wage~cbind(age, age^2, age^3, age^4), data = Wage)
coef(fit2b)
agelims <- range(age)
range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata=list(age.grid), se=T)
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age.grid, preds$fit, lwd=2, col="blue")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age.grid, preds$fit, lwd=1, col="blue")
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial")
lines(age.grid, preds$fit, lwd=1, col="darkblue")
matlines(age.grid, se.bands, lwd=1, col="lightblue", lty=3)
pred$fit
plot(age,wage,xlim=agelims,cex =.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="darkblue")
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,2,0))
plot(age,wage,xlim=agelims,cex =.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="darkblue")
fit=lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
fit2=lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
fit2a=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)
fit2b=lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
coef(fit2b)
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,2,0))
plot(age,wage,xlim=agelims,cex =.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="darkblue")
matlines(age.grid,se.bands,lwd=1,col="lightblue",lty=3)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
attach(Wage)
fit=lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
fit2=lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
fit2a=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)
fit2b=lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
coef(fit2b)
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,2,0))
plot(age,wage,xlim=agelims,cex =.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="darkblue")
matlines(age.grid,se.bands,lwd=1,col="lightblue",lty=3)
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2$fit ))
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60) ),data=Wage)
pred=predict(fit,newdata =list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2,col='#de2d26')
lines(age.grid,pred$fit+2*pred$se,lty="dashed",col='#fee0d2')
lines(age.grid,pred$fit-2*pred$se,lty="dashed",col='#fee0d2')
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots")
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred2$fit,col="#de2d26",lwd=2)
plot(age,wage,xlim=agelims,cex =.5,col="darkgrey")
title("Smoothing Spline")
fit=smooth.spline(age,wage,df=16)
fit2=smooth.spline(age,wage,cv=TRUE)
fit2$df
lines(fit,col="#de2d26",lwd =2)
lines(fit2,col="#3182bd",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("#de2d26","#3182bd"),lty=1,lwd=2, cex =.8)
plot(age,wage,xlim=agelims,cex =.5,col="darkgrey")
title("Local Regression")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="#de2d26",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="#3182bd",lwd=2)
legend("topright",legend=c("Span=0.2"," Span=0.5"),col=c("#de2d26","#3182bd"),lty=1,lwd=2, cex =.8)
library(gam)
install.packages("gam")
library(gam)
install.packages("akima")
library(akima)
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow=c(1,3))
plot(gam.m3, se=TRUE,col ="#1c9099")
par(mfrow=c(1,3))
plot.Gam(gam1, se=TRUE, col="#f03b20")
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
anova(gam.m1,gam.m2,gam.m3,test="F")
summary(gam.m3)
preds=predict(gam.m2,newdata=Wage)
gam.lo=gam(wage~s(year,df=4)+lo(age,span =0.7)+education,data=Wage)
par(mfrow=c(1,3))
plot.Gam(gam.lo, se=TRUE, col ="#31a354")
gam.lo.i=gam(wage~lo(year,age, span=0.5)+education,data=Wage)
par(mfrow=c(1,2))
plot(gam.lo.i,col='#bdbdbd')
gam.lr=gam(I(wage >250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="#31a354")
table(education,I(wage >250))
gam.lr.s=gam(I(wage >250)~year+s(age,df=5)+education,family=binomial,data=Wage, subset=(education !="1. < HS Grad"))
par(mfrow=c(1,3))
plot(gam.lr.s,se=T,col="#31a354")
summary(gam.m3)
preds=predict(gam.m2,newdata=Wage)
gam.lo=gam(wage~s(year,df=4)+lo(age,span =0.7)+education,data=Wage)
par(mfrow=c(1,3))
plot.Gam(gam.lo, se=TRUE, col ="#31a354")
gam.lo.i=gam(wage~lo(year,age, span=0.5)+education,data=Wage)
par(mfrow=c(1,2))
plot(gam.lo.i,col='#bdbdbd')
gam.lr=gam(I(wage >250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="#31a354")
table(education,I(wage >250))
gam.lr.s=gam(I(wage >250)~year+s(age,df=5)+education,family=binomial,data=Wage, subset=(education !="1. < HS Grad"))
par(mfrow=c(1,3))
plot(gam.lr.s,se=T,col="#31a354")
?gam
attach(College)
set.seed(10)
test_sample <- sample(1:nrow(College), nrow(College)/4)
train <- College[-test_sample, ]
test <- College[test_sample, ]
?leaps
install.packages("leaps")
library(leaps)
library(gam)
library(splines)
set.seed(10)
test_sample <- sample(1:nrow(College), nrow(College)/4)
train <- College[-test_sample, ]
test <- College[test_sample, ]
fwd <- regsubsets(Outstate ~ ., data=train, nvmax=17, method='forward')
fwd_sum <- summary(fwd)
par(mfrow=c(2,2))
plot(fwd_sum$cp ,xlab="Number of Variables ", ylab="Cp",
type="b")
points(which.min(fwd_sum$cp), fwd_sum$cp[which.min(fwd_sum$cp)], col="red", cex=2, pch=20)
plot(fwd_sum$bic ,xlab="Number of Variables ",
ylab="BIC",type="b")
points(which.min(fwd_sum$bic), fwd_sum$bic[which.min(fwd_sum$bic)], col="red", cex=2, pch=20)
plot(fwd_sum$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted R^2^",type="b")
points(which.max(fwd_sum$adjr2), fwd_sum$adjr2[which.max(fwd_sum$adjr2)], col="red", cex=2, pch=20)
which.min(fwd_sum$cp)
which.min(fwd_sum$bic)
which.max(fwd_sum$adjr2)
test_matrix <- model.matrix(Outstate~., data=test)
val.errors <- rep(NA,17)
for(i in 1:17){
coefi <- coef(fwd,id=i)
pred <- test_matrix[,names(coefi)]%*%coefi
val.errors[i] <- mean((test$Outstate-pred)^2)
}
which.min(val.errors)
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[12], col='red', pch=20, cex=2)
fwd_full <- regsubsets(Outstate ~ ., data=College, nvmax=17, method='forward')
coef(fwd_full, 6)
gam_fit <- gam(Outstate ~ Private + s(Room.Board, 3) + s(Terminal, 3) + s(perc.alumni, 3) + s(Expend, 3) + s(Grad.Rate, 3), data=train)
par(mfrow=c(2,3))
plot(gam_fit, se=TRUE, col="blue")
preds <- predict(gam_fit, newdata = test)
error <- mean((test$Outstate-preds)^2)
val.errors[6]-error
summary(gam_fit)
attach(Wage)
set.seed(1)
cv.error <- rep(0,5)
for (i in 1:5){
glm.fit <- glm(wage ~ poly(age,i),data=Wage)
cv.error[i]<- cv.glm(Wage,glm.fit,K=10)$delta[1]
}
cv.error
plot(cv.error, type="b", xlab="Degree", ylab="Test MSE")
points(which.min(cv.error), cv.error[4], col="red", pch=20, cex=2)
fit_1 <- lm(wage ~ age, data=Wage)
fit_2 <- lm(wage ~ poly(age, 2), data=Wage)
fit_3 <- lm(wage ~ poly(age, 3), data=Wage)
fit_4 <- lm(wage ~ poly(age, 4), data=Wage)
fit_5 <- lm(wage ~ poly(age, 5), data=Wage)
anova(fit_1, fit_2, fit_3, fit_4, fit_5)
age_lim <- range(age)
age_grid <- seq(from=age_lim[1], to=age_lim[2])
preds <- predict(fit_4, newdata=list(age=age_grid),se=TRUE)
se_bands <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
plot(age, wage, xlim=age_lim, cex=.5, col="darkgrey")
lines(age_grid, preds$fit, lwd=2, col="blue")
matlines(age_grid, se_bands, lwd=1, col="blue", lty=3)
set.seed(2)
cv.errors <- rep(NA, 10)
for(i in 2:10){
Wage$age.cut <- cut(Wage$age,i)
glm.fit <- glm(wage ~ age.cut, data=Wage)
cv.errors[i] <- cv.glm(Wage, glm.fit, K=10)$delta[1]
}
cv.errors
plot(2:10, cv.errors[-1], type="b", xlab="Number of cuts", ylab="Test MSE")
points(which.min(cv.errors), cv.errors[which.min(cv.errors)], col="red", pch=20, cex=2)
fit_step <- glm(wage ~ cut(age, 8), data=Wage)
preds <- predict(fit_step, data.frame(age = age_grid))
plot(age, wage, col="darkgray")
lines(age_grid, preds, col="darkgreen", lwd=2)
set.seed(10)
test_sample <- sample(1:nrow(College), nrow(College)/4)
train <- College[-test_sample, ]
test <- College[test_sample, ]
fwd <- regsubsets(Outstate ~ ., data=train, nvmax=17, method='forward')
fwd_sum <- summary(fwd)
par(mfrow=c(2,2))
plot(fwd_sum$cp ,xlab="Number of Variables ", ylab="Cp",
type="b")
points(which.min(fwd_sum$cp), fwd_sum$cp[which.min(fwd_sum$cp)], col="red", cex=2, pch=20)
plot(fwd_sum$bic ,xlab="Number of Variables ",
ylab="BIC",type="b")
points(which.min(fwd_sum$bic), fwd_sum$bic[which.min(fwd_sum$bic)], col="red", cex=2, pch=20)
plot(fwd_sum$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted R^2^",type="b")
points(which.max(fwd_sum$adjr2), fwd_sum$adjr2[which.max(fwd_sum$adjr2)], col="red", cex=2, pch=20)
which.min(fwd_sum$cp)
which.min(fwd_sum$bic)
which.max(fwd_sum$adjr2)
test_matrix <- model.matrix(Outstate~., data=test)
val.errors <- rep(NA,17)
for(i in 1:17){
coefi <- coef(fwd,id=i)
pred <- test_matrix[,names(coefi)]%*%coefi
val.errors[i] <- mean((test$Outstate-pred)^2)
}
which.min(val.errors)
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[12], col='red', pch=20, cex=2)
set.seed(10)
test_sample <- sample(1:nrow(College), nrow(College)/4)
train <- College[-test_sample, ]
test <- College[test_sample, ]
fwd <- regsubsets(Outstate ~ ., data=train, nvmax=17, method='forward')
fwd_sum <- summary(fwd)
par(mfrow=c(2,2))
plot(fwd_sum$cp ,xlab="Number of Variables ", ylab="Cp",
type="b")
points(which.min(fwd_sum$cp), fwd_sum$cp[which.min(fwd_sum$cp)], col="red", cex=2, pch=20)
plot(fwd_sum$bic ,xlab="Number of Variables ",
ylab="BIC",type="b")
points(which.min(fwd_sum$bic), fwd_sum$bic[which.min(fwd_sum$bic)], col="red", cex=2, pch=20)
plot(fwd_sum$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted R^2^",type="b")
points(which.max(fwd_sum$adjr2), fwd_sum$adjr2[which.max(fwd_sum$adjr2)], col="red", cex=2, pch=20)
which.min(fwd_sum$cp)
which.min(fwd_sum$bic)
which.max(fwd_sum$adjr2)
test_matrix <- model.matrix(Outstate~., data=test)
val.errors <- rep(NA,17)
for(i in 1:17){
coefi <- coef(fwd,id=i)
pred <- test_matrix[,names(coefi)]%*%coefi
val.errors[i] <- mean((test$Outstate-pred)^2)
}
which.min(val.errors)
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[13], col='red', pch=20, cex=2)
fwd_full <- regsubsets(Outstate ~ ., data=College, nvmax=17, method='forward')
coef(fwd_full, 5)
gam_fit <- gam(Outstate ~ Private + s(Room.Board, 3) + s(Terminal, 3) +
s(perc.alumni, 3) + s(Expend, 3) + s(Grad.Rate, 3), data=train)
par(mfrow=c(2,3))
plot(gam_fit, se=TRUE, col="blue")
preds <- predict(gam_fit, newdata = test)
error <- mean((test$Outstate-preds)^2)
val.errors[5]-error
summary(gam_fit)
install.packages("rpart")
library(ISLR)
library(caret)
library(caret)
library(rpart)
attach(Sales)
attach(sales)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
High <- as.factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
table(High)
Carseats <- data.frame(Carseats[,-1], High)
str(Carseats)
tree.carseats <- rpart(High~., data=Carseats, method="class", control=rpart.control(minsplit = 15, cp=0.1))
summary(tree.carseats
summary(tree.carseats)
summary(tree.carseats)
install.packages("rattle")
library(rattle)
fancyRpartPlot(tree.carseats)
tree.carseats
printcp(tree.carseats)
plotcp(tree.carseats)
tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"],"CP")]
tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"]
carseats.prune <- prune(tree.carseats, cp=tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"])
fancyRpartPlot(carseats.prune)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(MASS)
str(Boston)
train_control <- trainControl(method="repeatedcv", number=10, repeats = 3)
tree.boston <- train(medv~., data=Boston, trControl=train_control, method = "rpart")
tree.boston
tree.boston$finalModel
library(rpart.plot)
rpart.plot(tree.boston$finalModel)
inTrain <- createDataPartition(Boston$medv~., p=0.5, list=F)
inTrain <- createDataPartition(Boston$medv, p=0.5, list=F)
train <- Boston[inTrain,]
boston.rf <- train(medv~., data=train, method="rf", trControl=trainControl("cv", number=10), importance=T)
boston.rf <- train(medv~., data=train, method="rf", trControl=trainControl("cv", number=10), importance=T)
boston.rf$bestTune
boston.rf$finalModel
varImp(boston.rf)
plot(varImp(boston.rf))
gbm.boston <- train(medv~., data=train, distribution = "gaussian", method = "gbm",
trControl = train_control, verbose=F, metric = "RMSE", bag.fraction = 0.75)
train_control <- trainControl(method="cv", number = 10)
gbm.boston <- train(medv~., data=train, distribution = "gaussian", method = "gbm",
trControl = train_control, verbose=F, metric = "RMSE", bag.fraction = 0.75)
print(gbm.boston)
summary(gbm.boston)
par(mfrow=c(1,2))
plot(gbm.boston$finalModel, i="rm")
plot(gbm.boston$finalModel, i="lstat")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(tidyverse)
library(rpart.plot)
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.1)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.01)
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.001)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
p <- seq(0, 1, 0.01)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- tree(Sales ~ ., data = car_train)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
car_tree <- rpart(carseats$Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
car_tree <- rpart(Carseats$Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
attach(Carseats)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(rpart)
library(rattle)
library(tidyverse)
library(rpart.plot)
p <- seq(0, 1, 0.1)
gini_index <- 2*p*(1-p)
class_error <- 1 - pmax(p, 1-p)
cross_entropy <- -(p * log(p) + (1 - p) * log(1 - p))
matplot(p, cbind(gini_index, class_error, cross_entropy), col = c("red", "green", "blue"))
set.seed(1)
attach(Carseats)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
car_train <- Carseats[train,]
car_test <- Carseats[-train,]
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.1))
summary(car_tree)
High <- as.factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
table(High)
Carseats <- data.frame(Carseats[,-1], High)
str(Carseats)
##fit the tree
#cp = complexity parameter
tree.carseats <- rpart(High~., data=Carseats, method="class", control=rpart.control(minsplit = 15, cp=0.1))
summary(tree.carseats)
fancyRpartPlot(tree.carseats)
tree.carseats
printcp(tree.carseats)
plotcp(tree.carseats)
tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"]
carseats.prune <- prune(tree.carseats, cp=tree.carseats$cptable[which.min(tree.carseats$cptable[,"xerror"]),"CP"])
fancyRpartPlot(carseats.prune)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.01))
summary(car_tree)
car_tree <- rpart(Sales ~ ., data = Carseats, method = "class", control = rpart.control(minsplit = 15, cp = 0.01))
summary(car_tree)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 15, cp = 0.01))
summary(car_tree)
car_tree <- rpart(Sales ~ ., data = car_train, method = "class", control = rpart.control(minsplit = 10, cp = 0.01))
summary(car_tree)
